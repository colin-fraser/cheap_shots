---
title: Model Averaging in Genomics?
author: Drew Tyre
date: '2019-02-25'
draft: yes
categories:
  - Research
tags_include:
  - reply
  - statistics
slug: model-averaging-genomics
---

Over a year ago I received an email about my model averaging post:

> I came across your blog post https://drewtyre.rbind.io/post/rebutting_cade/ searching for information about model combination and model averaging.

> I work in human genomics, developing risk scores for predicting disease based on genetic markers (SNPs). We have now run into this ‘problem’ where we have multiple such scores from different sources, that when assessed in a regression model (cox or logistic) are all highly significant at the same time and their effect size is not completely attenuated by the inclusion of the other predictions (I am assessing the association of each score, which is a vector, with a specific disease outcome, say binary). Each score is standardised to zero-mean and unit variance.

> This observation has lead us to try combining these different predictions into one ‘composite’ prediction, which seems to substantially outperform any one of the individual predictions (i.e., combining three weakly-correlated predictions, with hazard ratios of 1.3, 1.4, and 1.5, respectively, gives a hazard ratio of 1.8 for the composite score). I believe this is due to Jensen’s inequality, as you explain in your blog.

> My question is, are you aware of good mainstream literature on this topic explaining why combining predictions is a good idea? In statistical genetics and human genomics this practice has not caught on so the concept seems like voodoo to them.

My answer then: 

I’m not that familiar with genomics data, although I know big sample sizes and many predictors create unique problems!

It’s not clear to me what is meant by a composite prediction. When model averaging it would be unlikely for the odds ratio of the prediction to increase over what the individual models predict. Maybe impossible.

In any case, from a Bayesian perspective [Hooten et al.](https://projecteuclid.org/euclid.ss/1009212519) is the go to source, and for more details on using AIC I would look at
[Burnham and Anderson](http://www.springer.com/us/book/9780387953649).

But, is it impossible? 

Two scenarios about reality. In one, there is a small set of weak predictors, 
all the others are zero, and all of the predictors are in the model set.
In the second scenario many variables matter with exponentially decreasing effect
sizes, and a random sample of the variables are included in the model set. 

```{r intro, message=FALSE, warning=FALSE}
library(tidyverse)
library(AICcmodavg) # for convienence
s1 <- function(N = 100, M = 10, effects = c(1, 0.5, 0.25)){
  x <- MASS::mvrnorm(n = N, mu = rep(0,M), Sigma = diag(M))
  effects <- c(effects, rep(0, M-length(effects)))
  logity <- x %*% effects
  y <- rbinom(N, size = 1, prob = 1/(1 + exp(-logity)))
  return(tibble(y = y, x = x))
}
glm(y~x, family = binomial, data =s1())
```

```{r}
s2 <- function(N = 100, M = 10, effects = list(e0 = 1, rate = -0.9)){
  x <- MASS::mvrnorm(n = N, mu = rep(0,M), Sigma = diag(M))
  effects <- with(effects, e0 * exp(rate * 0:(M-1)))
  logity <- x %*% effects
  y <- rbinom(N, size = 1, prob = 1/(1 + exp(-logity)))
  return(tibble(y = y, x = x)) # experiment -- this puts a matrix in a column
}
glm(y~x, family = binomial, data =s2())

```

`s1()` generates data according to the view of reality behind BIC. `s2()` generates
data according to the view of reality behind AIC. Now I need a plausible model
set. I don't know what genomics people do, but I imagine they test individual
genes first. I will also put in a null model to bound the set. Normally I would 
put in a global model, but if there are hundreds or thousands of genes that would
be impractical.

```{r}
models <- list(y~x[,1], # this is a weird construction but it works
               y~x[,2],
               y~x[,3],
               y~x[,4],
               y~x[,5],
               y~x[,6],
               y~x[,7],
               y~x[,8],
               y~x[,9],
               y~x[,10],
               y~1)
set.seed(83849035)
df <- s1()
s1fits <- map(models, glm, data = df, family=binomial)
modnames <- map_chr(models, deparse)
knitr::kable(aictab(s1fits, modnames = modnames), digits = 2)
```

I had to play around with different random seeds to get a set that had all three "true" predictors in
the top set. 1 is always the top, but 2 and 3 often drop way down along with the
null model. So how does a model average prediction stack up against the top model? First i'll extract
the "hazard ratios", which I believe is just $e^{\beta_1}$

```{r}
hr <- map_dfr(s1fits[1:10], broom::tidy, conf.int = TRUE) %>% 
  filter(term != "(Intercept)") %>% 
  mutate(hr = exp(estimate),
         lcl = exp(conf.low),
         hcl = exp(conf.high),
         AICc = map_dbl(s1fits, AICc)[1:10])
ggplot(data = hr,
#       mapping = aes(x = fct_reorder(term, AICc), y = hr)) + 
       mapping = aes(x = AICc, y = hr)) + 
  geom_point() + 
  geom_errorbar(mapping = aes(ymin = lcl, ymax = hcl), width = 0.1) + 
  labs(x = "Term ordered by AICc",
       y = "Hazard Ratio") + 
  geom_hline(yintercept = 1, linetype = 2)
```

That's kind of a cool plot. The two predictors that give the strongest results are well separated
from the rest. Now, what is a "compound prediction"? If it is a model averaged prediction, I think
that has to be necessarily worse than the top predictor because the model averaging will shrink the 
prediction back towards zero. Also, I'm not really sure how to get a single hazard ratio. Reading 
between the lines, I wonder if they are doing something with the predictor variables, like using
the sum of the top predictors. I'll use the top 3 because they had hazard ratios significantly 
different from 1.

```{r}
df$compound <- with(df, x[,1] + x[,3] + x[,5]) # dplyr doesn't like matrix columns
s1cmpd <- glm(y~compound, data = df, family = binomial)
broom::tidy(s1cmpd, conf.int = TRUE) %>% 
  filter(term != "(Intercept)") %>% 
  mutate(hr = exp(estimate),
         lcl = exp(conf.low),
         hcl = exp(conf.high)) %>% 
  select(hr, lcl, hcl)
```

Hmmm. That's also worse than either of the single predictors. But it could be that it happens sometimes. 

I wonder what happens if I use the sum of the true predictors.

```{r}
df$true <- with(df, x[,1] + x[,2] + x[,3]) # dplyr doesn't like matrix columns
s1true<- glm(y~true, data = df, family = binomial)
broom::tidy(s1true, conf.int = TRUE) %>% 
  filter(term != "(Intercept)") %>% 
  mutate(hr = exp(estimate),
         lcl = exp(conf.low),
         hcl = exp(conf.high)) %>% 
  select(hr, lcl, hcl)
```

The top predictor has a hazard ratio of 2.23, so this is slightly worse. What about the
two variables that had AICc values far below the others?

```{r}
df$true <- with(df, x[,1] + x[,3]) # dplyr doesn't like matrix columns
s1true<- glm(y~true, data = df, family = binomial)
broom::tidy(s1true, conf.int = TRUE) %>% 
  filter(term != "(Intercept)") %>% 
  mutate(hr = exp(estimate),
         lcl = exp(conf.low),
         hcl = exp(conf.high)) %>% 
  select(hr, lcl, hcl)
```

So that *is* higher, but not by much. I guess I could get a single hazard ratio by calculating
a model averaged prediction and then using it in a glm. 

```{r}
```

