---
title: "COVID-19 and R Part III"
author: "Drew Tyre"
date: "`r Sys.Date()`"
output:
  word_document: default
  html_document:
    code_folding: hide
draft: yes
slug: COVID-19_and_R_III
tags_include:
- R
- OPD
- COVID19
categories: Research
---

In today's post I want to summarize some information about what to do (and what not to do)
in the face of the COVID-19 pandemic, and examine how my predictions are holding up.
I feel a little bit guilty because the topic is serious, and this data reflects 
real suffering, but I am learning a lot with this real-time forecasting. In my
normal work I only get one data point *per year*. Which means building up skill
in forecasting takes a loooong time. 

I've also realized I have a few different possible audiences with different needs.
There are intelligent and interested people like my mother who want the bottom line
explained but don't care how I get there. There are students who want to learn 
more about how to use R. And then there are some serious data nerds who want to
know about my every choice in the analysis and ask why I'm not doing y or z. So
I'm going to put the bottom line at the top, so that my mother doesn't have to 
scroll too far on her tablet (seriously, this was a conversation we just had).
So if you want the gory details scroll down.

# The bottom line

```{r, warning=FALSE, message=FALSE, echo=FALSE}
library("tidyverse")
library("lubridate")

jhu_url <- paste("https://raw.githubusercontent.com/CSSEGISandData/", 
                 "COVID-19/master/csse_covid_19_data/", 
                 "csse_covid_19_time_series/", 
                 "time_series_19-covid-Confirmed.csv", sep = "")

#jhu_wide <- read_csv(jhu_url) # just grab it once from github
# archive it! 
#save(jhu_wide, file = "data/jhu_wide_2020-03-15.Rda")
load("data/jhu_wide_2020-03-15.Rda")
us_confirmed_total <- jhu_wide %>% 
  rename(province = "Province/State", 
         country_region = "Country/Region") %>% 
  pivot_longer(-c(province, country_region, Lat, Long), 
               names_to = "Date", values_to = "cumulative_cases") %>% 
  filter(country_region == "US") %>% 
  mutate(Date = mdy(Date)) %>% 
#  mutate(Date = lubridate::mdy(Date) - lubridate::days(1)) %>% 
  arrange(province, Date)  %>% 
  # filter out state rows prior to march 8, and county rows after that. 
  filter(str_detect(province, ", ") & Date <= "2020-03-9" |
           str_detect(province, ", ", negate = TRUE) & Date > "2020-03-9") %>% 
  group_by(Date) %>% # then group by Date and sum
  summarize(cumulative_cases = sum(cumulative_cases)) %>% 
  ungroup() %>% 
  mutate(incident_cases = c(0, diff(cumulative_cases)))

p1 <- ggplot(data = filter(us_confirmed_total, Date > "2020-02-28", Date <= "2020-03-11"),
       mapping = aes(x = Date)) + 
#  geom_line(mapping = aes(y = incident_cases)) + 
  geom_point(mapping = aes(y = incident_cases)) + 
  scale_y_log10() + 
  geom_smooth(mapping = aes(y = incident_cases), method = "lm")

us_exp_model <- us_confirmed_total %>% 
  mutate(log_incident_cases = log10(incident_cases),    # transform the data
         day = as.numeric(Date - ymd("2020-02-27"))) %>% # get day relative to Feb 27
  filter(Date > "2020-02-28", Date <= "2020-03-11") %>% 
  lm(data = .,
     formula = log_incident_cases ~ day) 

predicted <- tibble(day = 14:20)
predicted_list <- predict(us_exp_model, newdata = predicted, se.fit = TRUE)
predicted$fit <- predicted_list$fit # this is klutzy, but I want to see the answer! 
predicted$se.fit <- predicted_list$se.fit
predicted <- predicted %>% 
  mutate(Date = ymd("2020-2-27") + day,
         pred_var = se.fit^2 + predicted_list$residual.scale^2,
         lpl = 10^(fit - sqrt(pred_var)*qt(0.975, df = 11)),
         upl = 10^(fit + sqrt(pred_var)*qt(0.975, df = 11)),
         fit = 10^fit)

p1 + 
  geom_line(data = predicted,
            mapping = aes(y = fit),
            linetype = 2) +
  geom_ribbon(data = predicted,
            mapping = aes(ymin = lpl, ymax = upl),
            alpha = 0.2) +
  geom_point(data = filter(us_confirmed_total, Date > "2020-03-11"),
             mapping = aes(y = incident_cases)) +
  labs(y = "Incident cases", title = "Total new reported cases per day in the USA",
       subtitle = "Exponential model in blue; predicted values with 95% prediction intervals with dashed line.",
       caption = paste("Source data from https://github.com/CSSEGISandData/COVID-19 downloaded ",
                       format(file.mtime("data/jhu_wide_2020-03-15.Rda"), usetz = TRUE),
                       "\n Copyright Andrew Tyre 2020. Distributed with MIT License.")
       )
```

Saturday's count of new cases in the USA was within my prediction interval.[^1]
The prediction interval is the wider ribbon of grey around
the dashed line. For any one day, if you imagine 100 different
copies of reality, 95 of the 100 would have new cases that fall
inside the grey ribbon. So this prediction is still
looking good for my model. However, I would also
expect that the points would fluctuate above and below
the dashed line. I'm assuming that the rate of
new cases per case is not changing. If the "social distancing"
is working to reduce the spread, then the number of new cases
should start to drop away from my prediction. This is sort of
what is shown in the figure, but not severe enough yet
to make me throw out my simple model. Yet.

My friend Brenda Pracheil (also a data nerd) offered up an interesting 
hypothesis for the lower number of cases reported yesterday: the weekend hypothesis. 
Fewer doctors, clinics etc. open on the weekend, so fewer people getting tested and 
reported. Too early to judge if that's happening or not, but I'll keep
it in mind! The point that is farthest off my fitted line is a Monday. 

My summary of things to do:

- Wash your hands especially after going out and touching things other people have touched. 
Seriously. Soap more effective than hand sanitizer. Use hand sanitizer if you can't get
soap and water, but as long as you don't touch your face after touching door knobs, etc. 
you can wait and use soap. 

- Don't bother with a mask unless you are already sick with some respiratory illness. Apparently
wearing a mask makes you *more* likely to touch your face, and that's bad. 

- If you can, work from home and regardless avoid businesses and activities where you'll end up closer 
than 1 m (3 feet) from others. This "social distancing" reduces the number of people you
come in contact with per day, which reduces the rate at which the virus spreads. This is 
not an all or nothing thing. Tomas Pueyo, clearly a fellow (but wealthier and more successful!) data nerd,
wrote [a great article that shows the effects of social distancing](https://medium.com/@tomaspueyo/coronavirus-act-today-or-people-will-die-f4d3d9cd99ca) (among other interesting things).
This chart in particular shows the benefit of social distancing: 
![chart 21c](https://miro.medium.com/max/2942/1*aXhYA5D5PdTFjCTbFv8zmg.png)
Reducing the number of people you come in contact with by 25% would postpone the epidemic peak by weeks, and reduce the size of the epidemic. A little social distancing goes a long way.

- Don't expect this to be over in 2 weeks. If you're exposed, and you isolate yourself, after 
2 weeks you can be sure you didn't get infected by that exposure. But the epidemic is going to 
go on for quite a while! Months. Which leads me to ... 

- Don't buy enough toilet paper for a year.  "Stocking up" reduces the
number of times you go to the store for supplies, not enable you to weather the apocalypse. 
If you're in a grocery store, and there is some toilet paper, buy a package even if you don't need it.
But don't buy 20, that's just being silly. And rushing around to every store in town looking for
toilet paper just increases your contact rate! 

- Don't panic. Yes, lots of people are going to get sick, but if you're healthy and under 60 you
are very unlikely to die or even need serious medical help. You should practice social distancing,
even so, because that helps the broader community including people that are more likely to suffer
severe disease from the virus. 

- Do offer to help elderly or unhealthy neighbors and relatives. Offer to bring them supplies so 
they don't have to risk going out. If you do that, when you deliver keep your distance! You don't
want to be the vector for someone getting ill. 

Stay well everyone, and wash your hands!

# Full data nerd below this point

[That article by Tomas Pueyo](https://medium.com/@tomaspueyo/coronavirus-act-today-or-people-will-die-f4d3d9cd99ca)
has some good points about the difference between reported cases (what I'm using) and true cases
(what I wish we had). The CDC does have a table of cases by date of infection, but it's a 
small fraction of the total cases reported. Nonetheless, I thought it might be 
instructive to run the exponential model on that data. Annnd, it's buried inside a JSON
file used by a javascript widget that makes the figure and displays the table. No HTML table to scrape. 

```{r, message=FALSE}
# if running this for the first time, uncomment (once!) all the lines from 
# <HERE> to 
# library("RJSONIO")
# library("RCurl")
# 
# # grab the data
# raw_data <- getURL("https://www.cdc.gov/coronavirus/2019-ncov/us-cases-epi-chart.json")
# # Then covert from JSON into a list in R
# data <- fromJSON(raw_data)$data$columns # the list item named "data" has what we want, in an item named columns
# # now put in a tibble, need to do some manual things to remove the name from the start of the data
# cdc_epi <- tibble(
#   Date = mdy(data[[1]][-1]), # grab first item in list, remove first element (the name)
#   incident_cases = as.numeric(data[[2]][-1]) # grab second item in list, remove first element
# )
# # archive it, so that I use the same data for these figures always!
# also means I don't hit the server with data requests every time I save the post.
# save(cdc_epi, file = "data/cdc_epi_2020-03-15.Rda")
# <HERE>
load(file = "data/cdc_epi_2020-03-15.Rda")

# plot it, just to see what it looks like
ggplot(data = cdc_epi,
       mapping = aes(x = Date, y = incident_cases)) + 
  geom_point() +
  geom_vline(xintercept = ymd("2020-03-05"), linetype = 2)
```

Cool! It worked ... OK, the vertical dashed line marks the point at which the CDC thinks
not all cases are yet reported, so we shouldn't model things after March 5th, yet.
I want to fit the same model as before, but the difficulty with that model is
that it doesn't handle zeros well -- $log_{10}(0) = -\infty$. In my 
current predictions I'm dodging that issue by only using data after
the number of incident cases is always positive. But I've been 
wondering how a glm with log link would do. Also need to choose an
error distribution; a Poisson distribution has only positive 
support and discrete values so I'll try that. 

```{r}
cdc_epi_trimmed <- filter(cdc_epi, Date < "2020-03-05") %>% 
  mutate(days = as.numeric(Date - ymd("2020-02-27"))) # use day relative to Feb 27, same as other model
cdc_epi_glm <- glm(incident_cases ~ days, data = cdc_epi_trimmed, family = poisson)
summary(cdc_epi_glm)
```
Wow. Daily growth rate is 0.14 day^-1^, almost exactly the same estimate as I get for the
model based on the total number of cases. That model is mildly overdispersed
(ratio of residual deviance to degrees of freedom is 
`r format(cdc_epi_glm$deviance / cdc_epi_glm$df.residual, digits = 2)`), so
how does a Negative Binomial model compare?

```{r}
library("mgcv") # switch to mgcv::gam to estimate overdispersion parameter
cdc_epi_glm2 <- gam(incident_cases ~ days, data = cdc_epi_trimmed, family = mgcv::nb)
summary(cdc_epi_glm2)
```

The AIC for this model is `r format(AIC(cdc_epi_glm2),digits = 2)` so moderately better than
the Poisson model. 

[^1]: The code for this post that is not shown is [on Github](https://raw.githubusercontent.com/rbind/cheap_shots/master/content/post/covid-19_in_R_II.Rmd).