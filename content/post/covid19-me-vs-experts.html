---
title: "Challenging predictions with data: me vs. experts"
author: "Drew Tyre"
date: '2020-04-06'
output:
  pdf_document: default
  html_document:
    code_folding: hide
  word_document: default
og_image: /post/covid19-me-vs-experts_files/figure-html/featured_image-1.png
draft: yes
slug: covid19-me-vs-experts
tags_include:
  - R
  - OPD
  - COVID19
categories: 
  - Research
---



<p><a href="https://fivethirtyeight.com/features/best-case-and-worst-case-coronavirus-forecasts-are-very-far-apart/">FiveThirtyEight</a>
is reporting on <a href="https://works.bepress.com/mcandrew/4/">a project generating consensus predictions from experts</a>.
The consensus prediction of total cases in the USA on Sunday April 5 was 386,500 (80% UI: 280,500-581,500). <a href="/post/covid19-different-perspective/">My prediction used a time series model</a>.
How did we do?</p>
<p><a href="/post/covid-19_and_r/">The first post in this series was March 13</a>, in case you’re just joining and
want to see the progression. I’m now posting the bottom line figures for each <a href="/post/covid19-bottom-line-usa/">state</a> and <a href="/post/covid19-bottom-line-canada/">province</a> on
the same post that just gets updated each day.</p>
<p>DISCLAIMER: These unofficial results are not peer reviewed, and should not be
treated as such. My goal is to learn about forecasting in real time,
how simple models compare with more complex models, and even how to compare
different models.<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a></p>
<pre class="r"><code>library(&quot;tidyverse&quot;)
library(&quot;lubridate&quot;)
library(&quot;broom&quot;)
library(&quot;forecast&quot;)

savefilename &lt;- &quot;data/api_all_2020-04-06.Rda&quot;

load(savefilename)

country_data &lt;- read_csv(&quot;data/countries_covid-19.csv&quot;) %&gt;% 
  mutate(start_date = mdy(start_date)) 

source(&quot;R/jhu_helpers.R&quot;)
source(&quot;R/plot_functions.R&quot;)


usa_by_state &lt;- list(
  confirmed = us_wide2long(api_confirmed_regions, &quot;United States&quot;),
  deaths = us_wide2long(api_deaths_regions, &quot;United States&quot;),
  # recovered data is a mess lots of missing values
  recoveries = us_wide2long(api_recoveries_regions, &quot;United States&quot;)
  ) %&gt;% 
  bind_rows(.id = &quot;variable&quot;) %&gt;% 
  pivot_wider(names_from = variable, values_from = cumulative_cases) 
usa_by_state2 &lt;- plot_active_case_growth(usa_by_state, country_data)$data

usa_total &lt;- usa_by_state2 %&gt;%
  group_by(Date) %&gt;% 
  summarize(confirmed_cases = sum(confirmed, na.rm = TRUE)) %&gt;% 
  mutate(log10_cc = log10(confirmed_cases),
         # match how autoplot is representing the x axis?
         Time = as.numeric(Date),
         n_obs = n(),
         Training = Date &lt;= &quot;2020-03-29&quot;)  
training_data &lt;- filter(usa_total, Training)
expert_consensus &lt;- tibble(
  Date = ymd(&quot;2020-04-05&quot;),
  fit = 386500,
  lpl = 280500,
  hpl = 581500
)
usat_ts &lt;- zoo::zoo(pull(training_data, log10_cc), pull(training_data, Date))
usat_ts_fit &lt;- auto.arima(usat_ts, seasonal = FALSE)
usat_forecast &lt;- forecast(usat_ts_fit, h = 14) %&gt;% 
  as_tibble() %&gt;% 
  mutate(Date = as.Date(max(training_data$Time) + 1:14, ymd(&quot;1970-01-01&quot;)),
         fit = 10^`Point Forecast`,
         l80pl = 10^`Lo 80`,
         h80pl = 10^`Hi 80`,         
         l95pl = 10^`Lo 95`,
         h95pl = 10^`Hi 95`)
ggplot(data = usa_total)  + scale_y_log10() +
  geom_point(mapping = aes(x = Date, y = confirmed_cases))+
  geom_line(data = usat_forecast,
            mapping = aes(x = Date, y = fit), linetype = 2)+
  geom_ribbon(data = usat_forecast,
              mapping = aes(x = Date, ymin = l80pl, ymax = h80pl), alpha = 0.2) +
    geom_ribbon(data = usat_forecast,
              mapping = aes(x = Date, ymin = l95pl, ymax = h95pl), alpha = 0.2) +
  geom_point(data = expert_consensus,
             mapping = aes(x = Date, y = fit), color = &quot;blue&quot;) +
  geom_errorbar(data = expert_consensus,
                mapping = aes(x = Date, ymin = lpl, ymax = hpl), size = 0.1, color = &quot;blue&quot;) +
        labs(y = &quot;Cumulative confirmed cases&quot;, 
           title = paste0(&quot;Cumulative confirmed cases in USA&quot;),
           x = &quot;Date&quot;,
           subtitle = &quot;Dashed line: ARIMA(1,1) forecast cases with (80%, 95%) prediction intervals. \nBlue point Expert Consensus (80% UI)&quot;,
           caption = paste(&quot;Source data: https://coviddata.github.io/coviddata/ downloaded &quot;,
                           format(file.mtime(savefilename), usetz = TRUE),
                           &quot;\n Unofficial, not peer reviewed results.&quot;,
                           &quot;\n Copyright Andrew Tyre 2020. Distributed with MIT License.&quot;))</code></pre>
<p><img src="/post/covid19-me-vs-experts_files/figure-html/featured_image-1.png" width="672" /></p>
<p>So the time series model that assumes the rate of growth is constant is too pessimistic.
That’s more or less what I expected. I didn’t think the exponential growth model would
succeed anywhere near as long as it has. The expert consensus was much closer. I’m guessing
there will be another expert consensus for next sunday, so I’m going to make a prediction for
then using the automatic time series modelling.</p>
<pre class="r"><code>usat_ts2 &lt;- zoo::zoo(pull(usa_total, log10_cc), pull(usa_total, Date))
usat_ts_fit2 &lt;- auto.arima(usat_ts2, seasonal = FALSE)
usat_forecast2 &lt;- forecast(usat_ts_fit2, h = 7) %&gt;% 
  as_tibble() %&gt;% 
  mutate(Date = as.Date(max(usa_total$Time) + 1:7, ymd(&quot;1970-01-01&quot;)),
         fit = 10^`Point Forecast`,
         l80pl = 10^`Lo 80`,
         h80pl = 10^`Hi 80`,         
         l95pl = 10^`Lo 95`,
         h95pl = 10^`Hi 95`)
ggplot(data = usa_total)  + scale_y_log10() +
  geom_point(mapping = aes(x = Date, y = confirmed_cases))+
  geom_line(data = usat_forecast2,
            mapping = aes(x = Date, y = fit), linetype = 2)+
  geom_ribbon(data = usat_forecast2,
              mapping = aes(x = Date, ymin = l80pl, ymax = h80pl), alpha = 0.2) +
    geom_ribbon(data = usat_forecast2,
              mapping = aes(x = Date, ymin = l95pl, ymax = h95pl), alpha = 0.2) +
  geom_point(data = expert_consensus,
             mapping = aes(x = Date, y = fit), color = &quot;blue&quot;) +
  geom_errorbar(data = expert_consensus,
                mapping = aes(x = Date, ymin = lpl, ymax = hpl), size = 0.1, color = &quot;blue&quot;) +
        labs(y = &quot;Cumulative confirmed cases&quot;, 
           title = paste0(&quot;Cumulative confirmed cases in USA&quot;),
           x = &quot;Date&quot;,
           subtitle = &quot;Dashed line: ARIMA(0,2,1) forecast cases with (80%, 95%) prediction intervals. \nBlue point Expert Consensus (80% UI)&quot;,
           caption = paste(&quot;Source data: https://coviddata.github.io/coviddata/ downloaded &quot;,
                           format(file.mtime(savefilename), usetz = TRUE),
                           &quot;\n Unofficial, not peer reviewed results.&quot;,
                           &quot;\n Copyright Andrew Tyre 2020. Distributed with MIT License.&quot;))</code></pre>
<p><img src="/post/covid19-me-vs-experts_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<p>So using all the data up to now, the automatic model selection picked quite a different
model, a moving average model. I can also see a significant problem with this
approach to forecasting these data – the cumulative number of cases cannot go down,
while the model assumes it can. I think the solution is to model the number of new cases
per day (yes, that’s where I started oh so long ago!). Then simulate from the
posterior of the forecasts, calculating the cumulative sums.</p>
<p>But that’s for another day.</p>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Code not shown in the post can be found <a href="https://raw.githubusercontent.com/rbind/cheap_shots/master/content/post/covid19-different-perspective.Rmd">on Github</a>.<a href="#fnref1" class="footnote-back">↩</a></p></li>
</ol>
</div>
